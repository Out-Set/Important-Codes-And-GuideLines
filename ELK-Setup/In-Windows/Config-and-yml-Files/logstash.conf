input {
    file {
		path => "C:/Users/hp/Downloads/Intg-War-With-Dash/logs/bitsflow-int-web.log"
        type => "syslog"
		start_position => "end"  # or end Optional, depending on your requirements
		sincedb_path => "NUL"          # Use "NUL" in Windows to avoid sincedb file issues
    }
}

output {
  stdout { codec => rubydebug }
  
  elasticsearch {
    hosts => ["http://localhost:9200"]
    data_stream => true   # Enable data streams
    data_stream_type => "logs" # Define the type, could also be "metrics" or other types
    data_stream_dataset => "integrationlog"  # Specify a dataset name
    data_stream_namespace => "default"  # You can change this to suit your needs
  }
}



#  --------------------------------------------------------------------------------------------------------------
#Note:
#start_position Options:
#beginning:
	#Description: When start_position is set to "beginning", Logstash starts reading the log file from the very first line. This is useful when you want to ensure that all log entries are processed from the start of the file.
	#Use Case:
		#When processing a new log file for the first time.
		#When you want to reprocess all logs after a pipeline restart or configuration change.
		#Ideal for development, testing, or when the log data is historical, and you want to process everything from scratch.

#end:
	#Description: When start_position is set to "end", Logstash begins reading from the end of the file. It means Logstash will only process new log entries that are appended after it starts running. This option is helpful for tailing live log files.
	#Use Case:
		#Useful for monitoring and processing real-time logs (like system logs or application logs).
		#Prevents Logstash from processing older log entries that were already handled or are not relevant.
	
